{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jGJaO9nhdFWH"
      ],
      "authorship_tag": "ABX9TyMsc2+3A6sMTGuNJRmw0VYn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrcCap/AnotherTest4PACS/blob/main/Mask_VDenseLayer%26Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing stuff"
      ],
      "metadata": {
        "id": "jGJaO9nhdFWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJgV0Z-3Fy5h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from torch.profiler import profile, ProfilerActivity, record_function\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F, init\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXzJMIYxF7zb"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEkiYCNMfGIG"
      },
      "outputs": [],
      "source": [
        "# defining transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,)), # Normalize the images\n",
        "    transforms.Lambda(lambda x: torch.flatten(x))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faKaeutrfRQW",
        "outputId": "e31cc076-d869-4c8e-afc3-b44cfb938cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:04<00:00, 2116509.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 558697.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3392018.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6833044.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': <torch.utils.data.dataloader.DataLoader at 0x7b52ddb00c10>,\n",
              " 'test': <torch.utils.data.dataloader.DataLoader at 0x7b52ddb01360>}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Step 3: Load the dataset\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "batch_size = 2048\n",
        "\n",
        "\n",
        "loaders = {\n",
        "    'train' : torch.utils.data.DataLoader(train_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1),\n",
        "\n",
        "    'test'  : torch.utils.data.DataLoader(test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1),\n",
        "}\n",
        "loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer definition"
      ],
      "metadata": {
        "id": "QeI9C1QodHnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_linear(x, weights, bias):\n",
        "    return torch.nn.functional.linear(x, weights, bias)\n",
        "\n",
        "def single_sample(noise, std, mean):\n",
        "    return torch.nn.functional.linear(noise, std, mean)\n",
        "\n",
        "def parallel_linear(x, weight_realizations, bias):\n",
        "    if x.dim() == 2:\n",
        "        return torch.vmap(single_linear, in_dims = (None, 0, None))(x, weight_realizations, bias)\n",
        "    return torch.vmap(single_linear, in_dims = (0, 0, None))(x, weight_realizations, bias)\n",
        "\n",
        "def parallel_sample(noise, std, means):\n",
        "    return torch.vmap(single_sample, in_dims = (None, None, 1), out_dims=1)(noise, std, means)"
      ],
      "metadata": {
        "id": "nHSpKvWvbOWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalDenseLayer(nn.Module):\n",
        "    def __init__(self, input_shape, output_shape, mask=None, multivariate=False, alpha=0.05) -> None:\n",
        "        super(VariationalDenseLayer, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.multivariate = multivariate\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.in_features = torch.prod(torch.tensor(input_shape)).item()\n",
        "        self.out_features = torch.prod(torch.tensor(output_shape)).item()\n",
        "\n",
        "        self.set_weights()\n",
        "\n",
        "\n",
        "    def set_weights(self, mask=None, mean_w_values=None, std_w_values=None):\n",
        "\n",
        "        if mask is None:\n",
        "            self.mask = torch.ones(self.out_features, self.in_features)\n",
        "            active_mean_w = None\n",
        "            active_std_w = None\n",
        "        else:\n",
        "            self.mask = mask\n",
        "            active_mean_w = torch.masked_select(self.mean_w, self.mask)\n",
        "            active_std_w = torch.masked_select(self.std_w, self.mask)\n",
        "\n",
        "        self.active_weights = int(self.mask.sum().item())\n",
        "        self.mean_w = Parameter(torch.empty(self.active_weights), requires_grad=True)\n",
        "        self.std_w = Parameter(torch.empty(self.active_weights), requires_grad=True)\n",
        "\n",
        "        if active_mean_w is None and mean_w_values is None:\n",
        "            self.mean_w.data.fill_(0.)\n",
        "        elif active_mean_w is None:\n",
        "            self.mean_w.data.fill_(mean_w_values)\n",
        "        else:\n",
        "            self.mean_w.data.copy_(active_mean_w)\n",
        "\n",
        "        if active_std_w is None and std_w_values is None:\n",
        "            self.std_w.data.fill_(0.)\n",
        "        elif active_std_w is None:\n",
        "            self.std_w.data.fill_(std_w_values)\n",
        "        else:\n",
        "            self.std_w.data.copy_(active_std_w)"
      ],
      "metadata": {
        "id": "YKB0j20noCC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io1_ol4Pc8fL"
      },
      "outputs": [],
      "source": [
        "class VariationalDenseLayer(nn.Module):\n",
        "    def __init__(self, input_shape, output_shape, mask = None, multivariate = False, alpha = 0.05) -> None:\n",
        "        super(VariationalDenseLayer, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.multivariate = multivariate\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.in_features = torch.prod(torch.tensor(input_shape)).item()\n",
        "        self.out_features = torch.prod(torch.tensor(output_shape)).item()\n",
        "\n",
        "        self.set_weights()\n",
        "\n",
        "        self.mask_cholesky = torch.ones((self.out_features, int(self.in_features*(self.in_features-1)/2)))\n",
        "        self.cholesky_idx = torch.nonzero(self.mask_cholesky, as_tuple=False)\n",
        "\n",
        "        self.active_cholesky = int(self.mask_cholesky.sum().item())\n",
        "        self.chol_w = Parameter(torch.empty(self.active_cholesky), requires_grad=self.multivariate)\n",
        "        self.idx_tril = torch.tril_indices(self.in_features, self.in_features,-1)\n",
        "\n",
        "        self.mean_b = Parameter(torch.empty(self.out_features), requires_grad=True)\n",
        "\n",
        "        self.mean_b.data.fill_(0.)\n",
        "        self.chol_w.data.fill_(0.001*self.multivariate)\n",
        "\n",
        "\n",
        "    def set_weights(self, mask=None, mean_w_values=None, std_w_values=None):\n",
        "        if mask is None:\n",
        "            self.mask = torch.ones(self.out_features, self.in_features, dtype=torch.bool)\n",
        "            active_mean_w = None\n",
        "            active_std_w = None\n",
        "        else:\n",
        "            self.mask = mask\n",
        "            active_mean_w = torch.masked_select(self.get_mean(), self.mask)\n",
        "            active_std_w = torch.masked_select(self.get_std(), self.mask)\n",
        "\n",
        "        self.mask_idx = torch.nonzero(self.mask, as_tuple=False)\n",
        "        self.active_weights = int(self.mask.sum().item())\n",
        "        self.mean_w = Parameter(torch.empty(self.active_weights), requires_grad=True)\n",
        "        self.std_w = Parameter(torch.empty(self.active_weights), requires_grad=True)\n",
        "\n",
        "        if active_mean_w is None and mean_w_values is None:\n",
        "            self.mean_w.data.fill_(0.)\n",
        "        elif active_mean_w is None:\n",
        "            self.mean_w.data.fill_(mean_w_values)\n",
        "        else:\n",
        "            self.mean_w.data.copy_(active_mean_w)\n",
        "\n",
        "        if active_std_w is None and std_w_values is None:\n",
        "            self.std_w.data.fill_(0.01)\n",
        "        elif active_std_w is None:\n",
        "            self.std_w.data.fill_(std_w_values)\n",
        "        else:\n",
        "            self.std_w.data.copy_(active_std_w)\n",
        "\n",
        "    def get_mean(self) -> torch.tensor:\n",
        "        weight_tensor = torch.zeros(self.out_features, self.in_features)\n",
        "        weight_tensor[self.mask_idx[:,0], self.mask_idx[:,1]] = self.mean_w\n",
        "        return weight_tensor\n",
        "\n",
        "    def get_std(self) -> torch.tensor:\n",
        "        std_tensor = torch.zeros(self.out_features, self.in_features)\n",
        "        std_tensor[self.mask_idx[:,0], self.mask_idx[:,1]] = self.std_w\n",
        "        return std_tensor\n",
        "\n",
        "    def get_chol(self)-> torch.tensor:\n",
        "        cholesky_tril = torch.zeros(self.out_features, self.in_features, self.in_features)\n",
        "        cholesky_tril[:, range(self.in_features),range(self.in_features)] = self.get_std()\n",
        "        cholesky_tril[:, self.idx_tril[0], self.idx_tril[1]] = self.chol_w.view(self.out_features, int(self.in_features*(self.in_features-1)/2))\n",
        "        return cholesky_tril\n",
        "\n",
        "    def significant_weights(self) -> torch.Tensor:\n",
        "        z_score = torch.abs(self.get_mean()) / torch.sqrt(self.get_std() ** 2)\n",
        "        return (z_score > norm.ppf(1 - self.alpha/2))\n",
        "\n",
        "    def update_mask(self):\n",
        "        self.set_weights(mask=self.significant_weights())\n",
        "\n",
        "    def sample_weights(self) -> torch.Tensor:\n",
        "        if self.multivariate:\n",
        "            return torch.einsum(\"ij, oij -> oi\", torch.randn(self.in_features, self.in_features), self.get_chol()) + self.get_mean()\n",
        "        return torch.einsum(\"i, oi -> oi\", torch.randn(self.in_features), self.get_std()) + self.get_mean()\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        return torch.nn.functional.linear(x, self.sample_weights(), self.mean_b)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def single_linear(x, weights, bias):\n",
        "    return torch.nn.functional.linear(x, weights, bias)\n",
        "\n",
        "def parallel_linear(x, weight_realizations, bias):\n",
        "    if x.dim() == 2:\n",
        "        return torch.vmap(single_linear, in_dims = (None, 0, None))(x, weight_realizations, bias)\n",
        "    return torch.vmap(single_linear, in_dims = (0, 0, None))(x, weight_realizations, bias)"
      ],
      "metadata": {
        "id": "_79VNIoaz6lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalNet(nn.Module):\n",
        "    def __init__(self, layer_sizes, layer_masks = None) -> None:\n",
        "        super(VariationalNet, self).__init__()\n",
        "        # Store the actual shapes\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.layer_masks = layer_masks if layer_masks is not None else [None] * (len(layer_sizes) - 1)\n",
        "\n",
        "        # Initialize layers with corresponding masks\n",
        "        self.layers = nn.ModuleList([\n",
        "            VariationalDenseLayer(in_size, out_size, mask, multivariate=False)\n",
        "            for (in_size, out_size), mask in zip(zip(self.layer_sizes[:-1], self.layer_sizes[1:]), self.layer_masks)\n",
        "        ])\n",
        "\n",
        "    def realize_layers(self, num_samples):\n",
        "        def sample_all_layers(sample_index):\n",
        "            return [layer.sample_weights() for layer in self.layers]\n",
        "        # Use vmap to vectorize over samples\n",
        "        return torch.vmap(sample_all_layers, randomness='different')(torch.arange(num_samples))\n",
        "\n",
        "    def update_masks(self):\n",
        "        for layer in self.layers:\n",
        "            layer.update_mask()\n",
        "\n",
        "    def forward(self, x, num_samples):\n",
        "        # First, realize the weights\n",
        "        realized_weights = self.realize_layers(num_samples)\n",
        "        for layer, realized_layer in zip(self.layers, realized_weights):\n",
        "            x = parallel_linear(x, realized_layer, layer.mean_b)\n",
        "            if layer != self.layers[-1]:\n",
        "                x = torch.nn.LeakyReLU(negative_slope=0.1)(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "kEb4wRhr0ep4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_weights_and_covariance(means, chol, in_features, out_features):\n",
        "    # Reshape weights_mu into 28x28 images\n",
        "    img_size = int(np.sqrt(in_features))\n",
        "    covariance_matrices = torch.bmm(chol, chol.transpose(1, 2))\n",
        "\n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(out_features, 2, figsize=(10, out_features * 5))\n",
        "\n",
        "    for i in range(out_features):\n",
        "        # Plot weights_mu\n",
        "        axes[i, 0].imshow(means[i].view(img_size, img_size).cpu().detach().numpy(), cmap='viridis')\n",
        "        axes[i, 0].set_title(f'Weights (Output Neuron {i+1})')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Plot covariance matrix\n",
        "        axes[i, 1].imshow(covariance_matrices[i].cpu().detach().numpy(), cmap='viridis')\n",
        "        axes[i, 1].set_title(f'Covariance Matrix (Output Neuron {i+1})')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PfUGLLO949xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_weights(means):\n",
        "    # Reshape weights_mu into 28x28 images\n",
        "    img_size = int(np.sqrt(means.shape[0]))\n",
        "    out_features = means.shape[1]\n",
        "\n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(out_features, figsize=(10, out_features * 5))\n",
        "\n",
        "    for i in range(out_features):\n",
        "        # Plot weights_mu\n",
        "        axes[i].imshow(means[:,i].view(img_size, img_size).cpu().detach().numpy(), cmap='viridis')\n",
        "        axes[i].set_title(f'Weights (Output Neuron {i+1})')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "39hzwRhQLEzC"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "def parallel_loss(logits, target):\n",
        "    def single_loss(logit):\n",
        "        return torch.nn.functional.cross_entropy(logit, target, reduction='none')\n",
        "    return torch.vmap(single_loss, in_dims=0)(logits)\n",
        "\n",
        "def train(model, train_loader, val_loader, num_epochs, device, num_samples=10, update_masks = False, lr = 0.001):\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        print(f\"Epoch {epoch_idx+1}/{num_epochs}\")\n",
        "        print(\"Total trainable params:\" + str(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
        "\n",
        "        # Training\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=True)\n",
        "\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader_tqdm):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            batch_size = data.size(0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with multiple weight realizations\n",
        "            output = model(data, num_samples)  # Shape: [num_samples, batch_size, num_classes]\n",
        "\n",
        "            # Compute parallel loss\n",
        "            losses = parallel_loss(output, target)  # Shape: [num_samples, batch_size]\n",
        "\n",
        "            # Average loss over samples and batch\n",
        "            loss = losses.mean()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_size\n",
        "\n",
        "            # Compute accuracy using the mean prediction over samples\n",
        "            mean_output = output.mean(dim=0)\n",
        "            pred = mean_output.argmax(dim=1, keepdim=True)\n",
        "            train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total_train_samples += batch_size\n",
        "\n",
        "            # Calculate batch accuracy and average loss\n",
        "            batch_acc = 100. * train_correct / total_train_samples\n",
        "            batch_loss = train_loss / total_train_samples\n",
        "\n",
        "            # Update tqdm progress bar with current loss and accuracy\n",
        "            train_loader_tqdm.set_postfix({'Loss': f'{batch_loss:.4f}', 'Acc': f'{batch_acc:.2f}%'})\n",
        "\n",
        "        train_loss /= total_train_samples\n",
        "        train_acc = 100. * train_correct / total_train_samples\n",
        "        if update_masks:\n",
        "            print(\"Updating masks...\")\n",
        "            model.update_masks()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "2zF9nxY9xjnr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = loaders['train']\n",
        "model = VariationalNet(layer_sizes=[784, 128, 64, 32, 10])\n",
        "\n",
        "print('TRAINING + MASKING')\n",
        "train(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=None,\n",
        "    num_epochs=10,\n",
        "    device=torch.device(\"cpu\"),\n",
        "    num_samples=100,\n",
        "    update_masks=True,\n",
        "    lr = 0.01\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKY5um0Cwy6I",
        "outputId": "a39207ab-b0b6-438c-96b9-207ccf6cc1c8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING + MASKING\n",
            "Epoch 1/10\n",
            "Total trainable params:222058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:31<00:00,  3.05s/it, Loss=2.0415, Acc=20.24%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 2.0415, Train Acc: 20.24%\n",
            "Epoch 2/10\n",
            "Total trainable params:51818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:31<00:00,  3.06s/it, Loss=1.1995, Acc=58.45%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 1.1995, Train Acc: 58.45%\n",
            "Epoch 3/10\n",
            "Total trainable params:35080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:31<00:00,  3.05s/it, Loss=0.6297, Acc=80.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.6297, Train Acc: 80.76%\n",
            "Epoch 4/10\n",
            "Total trainable params:28818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:30<00:00,  3.00s/it, Loss=0.3789, Acc=88.71%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.3789, Train Acc: 88.71%\n",
            "Epoch 5/10\n",
            "Total trainable params:25316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:29<00:00,  3.00s/it, Loss=0.2744, Acc=91.96%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.2744, Train Acc: 91.96%\n",
            "Epoch 6/10\n",
            "Total trainable params:23050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:31<00:00,  3.05s/it, Loss=0.2146, Acc=93.75%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.2146, Train Acc: 93.75%\n",
            "Epoch 7/10\n",
            "Total trainable params:21538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:38<00:00,  3.29s/it, Loss=0.1782, Acc=94.74%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.1782, Train Acc: 94.74%\n",
            "Epoch 8/10\n",
            "Total trainable params:20390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:32<00:00,  3.07s/it, Loss=0.1537, Acc=95.48%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.1537, Train Acc: 95.48%\n",
            "Epoch 9/10\n",
            "Total trainable params:19354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:29<00:00,  2.99s/it, Loss=0.1398, Acc=95.84%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.1398, Train Acc: 95.84%\n",
            "Epoch 10/10\n",
            "Total trainable params:18628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [01:31<00:00,  3.04s/it, Loss=0.1287, Acc=96.12%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.1287, Train Acc: 96.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('TRAINING + MASKING')\n",
        "train(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=None,\n",
        "    num_epochs=3,\n",
        "    device=torch.device(\"cpu\"),\n",
        "    num_samples=200,\n",
        "    update_masks=True,\n",
        "    lr = 0.001\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfiXGBnY0q7Q",
        "outputId": "b6be43be-9c47-42c8-ea26-57f3afdcd9d6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING + MASKING\n",
            "Epoch 1/3\n",
            "Total trainable params:18034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [02:58<00:00,  5.96s/it, Loss=0.1051, Acc=96.87%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.1051, Train Acc: 96.87%\n",
            "Epoch 2/3\n",
            "Total trainable params:18006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [02:56<00:00,  5.87s/it, Loss=0.1013, Acc=96.94%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.1013, Train Acc: 96.94%\n",
            "Epoch 3/3\n",
            "Total trainable params:17988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 30/30 [02:57<00:00,  5.91s/it, Loss=0.0993, Acc=97.01%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating masks...\n",
            "Train Loss: 0.0993, Train Acc: 97.01%\n"
          ]
        }
      ]
    }
  ]
}